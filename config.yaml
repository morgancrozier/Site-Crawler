# Crawler Configuration

# Target website configuration
target:
  start_url: "https://example.com"
  max_pages: 100
  keyword: null  # Optional keyword for analysis

# Performance settings
performance:
  delay: 0.5  # Delay between requests in seconds
  max_workers: 5  # Maximum number of concurrent threads
  timeout: 30  # Request timeout in seconds
  batch_size: 50  # Database batch size

# Checkpoint configuration
checkpoint:
  enabled: true
  interval: 100  # Save checkpoint every N pages
  file: "crawler_checkpoint.json"
  keep_on_complete: false  # Whether to keep checkpoint file after successful completion

# Progress tracking
progress:
  track_speed: true  # Track crawl speed
  track_memory: true  # Track memory usage
  track_errors: true  # Track detailed error statistics
  track_content_types: true  # Track content type distribution
  track_status_codes: true  # Track HTTP status code distribution
  track_timing: true  # Track detailed timing statistics
  stats_interval: 60  # Update detailed stats every N seconds

# HTTP settings
http:
  headers:
    User-Agent: "SeoCrawler/1.0 (+https://example.com/crawler-info)"
    Accept: "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
    Accept-Language: "en-US,en;q=0.5"
  skip_patterns:
    - "\\.css$"
    - "\\.js$"
    - "\\.jpg$"
    - "\\.jpeg$"
    - "\\.png$"
    - "\\.gif$"
    - "\\.ico$"
    - "\\.pdf$"
    - "\\.doc$"
    - "\\.docx$"
    - "\\.zip$"
    - "\\.rar$"
    - "\\.xml$"

# Output settings
output:
  database_file: "crawl_results.db"
  csv_file: "crawl_results.csv"
  json_file: "crawl_results.json"
  report_file: "crawl_report.json"
  stats_file: "crawl_stats.json"  # New detailed statistics file
  max_headings_per_level: 5

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s [%(levelname)s] %(message)s"
  file: "crawler.log"  # Optional file logging